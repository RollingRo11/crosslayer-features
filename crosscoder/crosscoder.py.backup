import torch
import torch.nn as nn
import torch.nn.functional as F
from nnsight import LanguageModel
import wandb
import einops
from typing import NamedTuple
from pathlib import Path
import tqdm
import json
import numpy as np
from datasets import load_dataset
from torch.nn.utils import clip_grad_norm_
import sys
import torch.autograd as autograd
import math</parameter>
import torch.autograd as autograd

sys.path.append(str(Path(__file__).parent.parent / "sae_vis" / "sae_vis"))
from model_utils import get_layer_output

PROJECT_ROOT = Path(__file__).parent.parent
DATASET_CACHE_DIR = PROJECT_ROOT / "data" / "hf_datasets_cache"
DATASET_CACHE_DIR.mkdir(parents=True, exist_ok=True)


class LossOutput(NamedTuple):
    reconstruction_loss: torch.Tensor
    sparsity_loss: torch.Tensor
    pre_act_loss: torch.Tensor


def print_gpu_memory(tag=""):
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated(0) / (1024**3)
        reserved = torch.cuda.memory_reserved(0) / (1024**3)
        free, total = torch.cuda.mem_get_info(0)
        used = (total - free) / (1024**3)
        total_gb = total / (1024**3)

        print(f"--- GPU Memory {tag} ---")
        print(f"  - Allocated by Tensors: {allocated:.2f} GB")
        print(f"  - Reserved by PyTorch:  {reserved:.2f} GB")
        print(f"  - Total GPU Usage:      {used:.2f} GB / {total_gb:.2f} GB")
        print("--------------------------")


cc_config = {
    "seed": 11,
    "batch_size": 2048,
    "buffer_mult": 24,
    "lr": 2e-5,
    "num_tokens": int(4e8),
    "beta1": 0.9,
    "beta2": 0.999,
    "context": 4096,
    "device": "cuda",
    "model_batch_size": 8,
    "log_interval": 100,
    "save_interval": 50000,
    "model_name": "gemma2-2b",  # gpt2, pythia, gemma3-4b, qwen3-4b, gemma2-2b
    "dtype": torch.bfloat16,
    "ae_dim": 2**15,
    "drop_bos": True,
    "total_steps": 100000,
    "normalization": "layer_wise",
    "optimizer": "adamw",
    "dec_init_norm": 0.003,
    "l_s_coefficient": 10,
    "l_p_coefficient": 3e-6,
    "c": 4.0,
}

CROSSCODER_DIR = Path(__file__).parent
SAVE_DIR = CROSSCODER_DIR / "saves"
WANDB_DIR = CROSSCODER_DIR / "wandb"


# enc of n_layers, d_model, d_sae
# dec of d_sae, n_layers, d_model
# loss func (we gonna use L1 of norms)
# d_sae could be anyting
class Crosscoder(nn.Module):
    def __init__(self, cfg, model):
        super().__init__()
        self.cfg = cfg
        self.model = model
        self.modelcfg = self.model.config.to_dict()  # type: ignore
        self.context = self.cfg["context"]

        if "num_hidden_layers" in self.modelcfg:
            self.num_layers = self.modelcfg["num_hidden_layers"]
        elif "n_layer" in self.modelcfg:
            self.num_layers = self.modelcfg["n_layer"]
        elif "num_layers" in self.modelcfg:
            self.num_layers = self.modelcfg["num_layers"]
        else:
            raise ValueError(
                f"Could not find number of layers in model config. Available keys: {list(self.modelcfg.keys())}"
            )

        if "hidden_size" in self.modelcfg:
            self.resid_dim = self.modelcfg["hidden_size"]
        elif "n_embd" in self.modelcfg:
            self.resid_dim = self.modelcfg["n_embd"]
        elif "d_model" in self.modelcfg:
            self.resid_dim = self.modelcfg["d_model"]
        else:
            raise ValueError(
                f"Could not find hidden dimension in model config. Available keys: {list(self.modelcfg.keys())}"
            )

        self.init_norm = cfg["dec_init_norm"]
        self.seed = self.cfg["seed"]
        torch.manual_seed(self.seed)
        self.ae_dim = cfg["ae_dim"]
        self.dtype = cfg["dtype"]
        self.save_dir = None
        # Use log-space threshold parameterization as per January 2025 paper
        self.log_threshold = nn.Parameter(
            torch.full((self.ae_dim,), 0.1, dtype=self.dtype)
        )
        dec_init_norm = self.init_norm</parameter>
        dec_init_norm = self.init_norm

        self.W_enc = nn.Parameter(
            torch.empty(self.num_layers, self.resid_dim, self.ae_dim, dtype=self.dtype)
        )

        # Initialize W_dec with uniform distribution U(-1/√n, 1/√n) as per January 2025 paper
        self.W_dec = nn.Parameter(
            torch.empty(
                self.ae_dim, self.num_layers, self.resid_dim, dtype=self.dtype
            )
        )
        bound = 1.0 / math.sqrt(self.resid_dim)
        torch.nn.init.uniform_(self.W_dec, -bound, bound)

        self.W_dec.data = self.W_dec.data * (
            dec_init_norm / self.W_dec.data.norm(dim=-1, keepdim=True)
        )

        # Initialize W_enc with (n/m) scaling factor as per January 2025 paper
        scaling_factor = self.resid_dim / self.ae_dim
        self.W_enc.data = einops.rearrange(
            self.W_dec.data.clone(),
            "ae_dim n_layers d_model -> n_layers d_model ae_dim",
        ) * scaling_factor</parameter>
        )

        self.b_enc = nn.Parameter(torch.zeros(self.ae_dim, dtype=self.dtype))
        self.b_dec = nn.Parameter(
            torch.zeros((self.num_layers, self.resid_dim), dtype=self.dtype)
        )

        self.to(cfg["device"])

    def encode(self, x):
        if self.W_enc is None:
            w_enc_t = einops.rearrange(
                self.W_dec, "ae_dim n_layers d_model -> n_layers d_model ae_dim"
            )
        else:
            w_enc_t = self.W_enc

        x_enc = einops.einsum(
            x,
            w_enc_t,
            "... n_layers d_model, n_layers d_model ae_dim -> ... ae_dim",
        )

        preacts = x_enc + self.b_enc
        # Use exp(log_threshold) and bandwidth=2.0 as per January 2025 paper
        acts = JumpReLUFunction.apply(preacts, torch.exp(self.log_threshold), 2.0)</parameter>

        return preacts, acts

    def decode(self, acts):
        acts_dec = einops.einsum(
            acts,
            self.W_dec,
            "... ae_dim, ae_dim n_layers d_model -> ... n_layers d_model",
        )

        return acts_dec + self.b_dec

    def forward(self, x):
        acts = self.encode(x)
        return self.decode(acts)

    def return_loss(self, x):
        x = x.to(self.dtype).to(self.cfg["device"])

        preacts, acts = self.encode(x)

        reconstructed_x = self.decode(acts)

        squared_diff = (reconstructed_x.float() - x.float()).pow(2)
        reconstruction_loss = einops.reduce(
            squared_diff, "batch n_layers d_model -> batch", "sum"
        ).mean()

        decoder_norms_per_layer = self.W_dec.norm(dim=-1)  # Shape: (ae_dim, num_layers)
        decoder_norms = einops.reduce(
            decoder_norms_per_layer, "ae_dim n_layers -> ae_dim", "sum"
        )

        # Sparsity loss: decoder norms should be OUTSIDE tanh, not inside
        c = self.cfg["c"]
        sparsity_term = torch.tanh(c * torch.abs(acts)) * decoder_norms
        sparsity_loss = sparsity_term.sum(dim=-1).mean()

        # Pre-act loss: use exp(log_threshold) and acts (post-activation) as per January 2025 paper
        relu_term = F.relu(torch.exp(self.log_threshold) - acts)
        pre_act_loss_term = relu_term * decoder_norms
        pre_act_loss = pre_act_loss_term.sum(dim=-1).mean()</parameter>

        return LossOutput(
            reconstruction_loss=reconstruction_loss,
            sparsity_loss=sparsity_loss,
            pre_act_loss=pre_act_loss,
        )

    def save(self):
        if self.save_dir is None:
            SAVE_DIR.mkdir(parents=True, exist_ok=True)
            version_list = [
                int(file.name.split("_")[1])
                for file in list(SAVE_DIR.iterdir())
                if "version" in str(file)
            ]
            if len(version_list):
                version = 1 + max(version_list)
            else:
                version = 0
            self.save_dir = SAVE_DIR / f"version_{version}"
            self.save_dir.mkdir(parents=True, exist_ok=True)

        self.save_version = getattr(self, "save_version", 0)

        weight_path = self.save_dir / f"{self.save_version}.pt"
        cfg_path = self.save_dir / f"{self.save_version}_cfg.json"

        torch.save(
            {
                "W_enc": self.W_enc.data,
                "W_dec": self.W_dec.data,
                "b_enc": self.b_enc.data,
                "b_dec": self.b_dec.data,
                "log_threshold": self.log_threshold.data,
                "cfg": self.cfg,
            },</parameter>
            },
            weight_path,
        )

        cfg_to_save = self.cfg.copy()
        cfg_to_save["dtype"] = str(cfg_to_save["dtype"])

        with open(cfg_path, "w") as f:
            json.dump(cfg_to_save, f, indent=2)

        self.save_version += 1


class JumpReLUFunction(autograd.Function):
    @staticmethod
    def forward(ctx, x, threshold, bandwidth):
        ctx.save_for_backward(x, threshold, torch.tensor(bandwidth))
        return x * (x > threshold).float()

    @staticmethod
    def backward(ctx, grad_output):
        x, threshold, bandwidth_tensor = ctx.saved_tensors
        bandwidth = bandwidth_tensor.item()
        x_grad = (x > threshold).float() * grad_output
        threshold_grad = (
            -(1.0 / bandwidth)
            * RectangleFunction.apply((x - threshold) / bandwidth)
            * grad_output
        )
        return x_grad, threshold_grad, None


class RectangleFunction(autograd.Function):
    @staticmethod
    def forward(ctx, x):
        ctx.save_for_backward(x)
        return ((x > -0.5) & (x < 0.5)).float()

    @staticmethod
    def backward(ctx, grad_output):
        (x,) = ctx.saved_tensors
        grad_input = grad_output.clone()
        grad_input[(x <= -0.5) | (x >= 0.5)] = 0
        return grad_input


class Buffer:
    def __init__(self, cfg, model):
        self.cfg = cfg
        self.model = model
        self.context = self.cfg["context"]
        self.modelcfg = self.model.config.to_dict()  # type: ignore

        if "num_hidden_layers" in self.modelcfg:
            self.num_layers = self.modelcfg["num_hidden_layers"]
        elif "n_layer" in self.modelcfg:
            self.num_layers = self.modelcfg["n_layer"]
        elif "num_layers" in self.modelcfg:
            self.num_layers = self.modelcfg["num_layers"]
        else:
            raise ValueError(
                f"Could not find number of layers in model config. Available keys: {list(self.modelcfg.keys())}"
            )

        if "hidden_size" in self.modelcfg:
            self.resid_dim = self.modelcfg["hidden_size"]
        elif "n_embd" in self.modelcfg:
            self.resid_dim = self.modelcfg["n_embd"]
        elif "d_model" in self.modelcfg:
            self.resid_dim = self.modelcfg["d_model"]
        else:
            raise ValueError(
                f"Could not find hidden dimension in model config. Available keys: {list(self.modelcfg.keys())}"
            )

        self.buffer_size = self.cfg["batch_size"] * cfg["buffer_mult"]
        self.buffer_batches = self.buffer_size // (self.context - 1)
        self.buffer_size = self.buffer_batches * (self.context - 1)

        self.buffer = torch.zeros(
            (self.buffer_size, self.num_layers, self.resid_dim),
            dtype=torch.bfloat16,
            requires_grad=False,
        ).to(cfg["device"])
        self.pointer = 0
        self.first = True
        self.normalize = True

        self.dataset = load_dataset(
            "monology/pile-uncopyrighted",
            split="train",
            streaming=True,
            cache_dir=str(DATASET_CACHE_DIR),
        )
        self.dataset_iter = iter(self.dataset)

        # Single global normalization factor
        estimated_norm_scaling_factor = self.estimate_norm_scaling_factor(batch_size=2)
        self.normalisation_factor = torch.tensor(
            estimated_norm_scaling_factor,
            device=cfg["device"],
            dtype=torch.bfloat16,
        )</parameter>
        )

        self.refresh()

    @torch.no_grad()
    def estimate_norm_scaling_factor(
        self, batch_size, n_batches_for_norm_estimate: int = 100
    ):
        # Use single global scaling factor as per January 2025 paper:
        # "The dataset is scaled by a single constant such that E[||x||_2] = √n"
        all_norms = []

        token_generator = self.get_tokens_batch_generator(
            n_batches_for_norm_estimate, batch_size
        )

        for tokens in tqdm.tqdm(
            token_generator,
            total=n_batches_for_norm_estimate,
            desc="Estimating norm scaling factor",
        ):
            tokens = tokens.to(self.cfg["device"])

            all_acts = []
            for j in range(0, len(tokens), self.cfg["model_batch_size"]):
                batch_tokens = tokens[j : j + self.cfg["model_batch_size"]]

                with self.model.trace(batch_tokens) as tracer:
                    layer_outputs = []
                    for layer_idx in range(self.num_layers):
                        layer_out = get_layer_output(self.model, layer_idx, tracer)
                        layer_outputs.append(layer_out)

                batch_acts = torch.stack(layer_outputs, dim=2)

                if self.cfg.get("drop_bos", True):
                    batch_acts = batch_acts[:, 1:, :, :]

                batch_acts = batch_acts.reshape(-1, self.num_layers, self.resid_dim)
                all_acts.append(batch_acts)

            acts = torch.cat(all_acts, dim=0)
            # Compute norms across all layers (treating as single dataset)
            norms = acts.norm(dim=-1)  # Shape: (batch, num_layers)
            all_norms.append(norms.flatten())

        # Compute global mean norm across all samples and layers
        all_norms = torch.cat(all_norms)
        mean_norm = all_norms.mean().item()
        # Return single scaling factor: mean_norm / √n
        scaling_factor = mean_norm / math.sqrt(self.resid_dim)

        return scaling_factor</parameter>
        return scaling_factors

    def get_tokens_batch_generator(self, n_batches, batch_size):
        """
        A generator that yields batches of tokens.
        This avoids loading the entire dataset into memory.
        """
        batch_count = 0
        while batch_count < n_batches:
            tokens = []
            count = 0

            while count < batch_size:
                try:
                    item = next(self.dataset_iter)
                except StopIteration:
                    self.dataset_iter = iter(self.dataset)
                    item = next(self.dataset_iter)

                text = item["text"]
                if len(text.strip()) < 50:
                    continue

                token_ids = self.model.tokenizer.encode(
                    text,
                    return_tensors="pt",
                    max_length=self.context,
                    truncation=True,
                    padding="max_length",
                )
                tokens.append(token_ids)
                count += 1

            yield torch.cat(tokens, dim=0)
            batch_count += 1

    # In the Buffer class

    @torch.no_grad()
    def refresh(self):
        tokens = self.get_tokens_batch()
        self.buffer.zero_()
        pointer = 0

        for i in tqdm.tqdm(
            range(0, len(tokens), self.cfg["model_batch_size"]),
            desc="Refreshing Buffer",
        ):
            batch_tokens = tokens[i : i + self.cfg["model_batch_size"]].to(
                self.cfg["device"]
            )

            if self.cfg.get("drop_bos", True):
                num_tokens_in_batch = batch_tokens.shape[0] * (
                    batch_tokens.shape[1] - 1
                )
            else:
                num_tokens_in_batch = batch_tokens.shape[0] * batch_tokens.shape[1]

            if pointer + num_tokens_in_batch > self.buffer_size:
                break

            with self.model.trace(batch_tokens) as tracer:
                for layer_idx in range(self.num_layers):
                    layer_out = get_layer_output(self.model, layer_idx, tracer)

                    if self.cfg.get("drop_bos", True):
                        layer_out = layer_out[:, 1:, :]

                    reshaped_layer_out = layer_out.reshape(-1, self.resid_dim)

                    self.buffer[
                        pointer : pointer + num_tokens_in_batch, layer_idx, :
                    ] = reshaped_layer_out.to(self.cfg["dtype"])

            pointer += num_tokens_in_batch

        perm = torch.randperm(pointer, device=self.cfg["device"])
        self.buffer[:pointer] = self.buffer[:pointer][perm]
        self.pointer = 0
        torch.cuda.empty_cache()

    @torch.no_grad()
    def next(self):
        if self.pointer + self.cfg["batch_size"] > len(self.buffer):
            self.refresh()

        batch = self.buffer[
            self.pointer : self.pointer + self.cfg["batch_size"]
        ].float()
        self.pointer += self.cfg["batch_size"]

        if self.normalize:
            # Single global normalization factor
            batch = batch / self.normalisation_factor

        return batch.to(self.cfg["device"])</parameter>
        return batch.to(self.cfg["device"])

    def get_tokens_batch(self):
        tokens = []
        count = 0
        max_samples = self.buffer_batches

        while count < max_samples:
            try:
                item = next(self.dataset_iter)
            except StopIteration:
                self.dataset_iter = iter(self.dataset)
                continue  # Skip to the next iteration to get an item

            text = item["text"]
            if len(text.strip()) < 50:
                continue

            token_ids = self.model.tokenizer.encode(
                text,
                return_tensors="pt",
                max_length=self.context,
                truncation=True,
                padding="max_length",
            )
            tokens.append(token_ids)
            count += 1

        return torch.cat(tokens, dim=0) if tokens else torch.empty(0)


class Trainer:
    def __init__(self, cfg, use_wandb=True):
        self.cfg = cfg
        if self.cfg["model_name"] == "gpt2":
            self.model = LanguageModel("gpt2", device_map="auto")
        elif self.cfg["model_name"] == "pythia":
            self.model = LanguageModel(
                "EleutherAI/pythia-2.8b-deduped", device_map="auto"
            )
        elif self.cfg["model_name"] == "gemma3-4b":
            self.model = LanguageModel("google/gemma-2-9b", device_map="auto")
        elif self.cfg["model_name"] == "qwen3-4b":
            self.model = LanguageModel("Qwen/Qwen2.5-3B", device_map="auto")
        elif self.cfg["model_name"] == "gemma2-2b":
            self.model = LanguageModel("google/gemma-2-2b", device_map="auto")

        self.context = self.cfg["context"]
        self.crosscoder = Crosscoder(cfg, model=self.model)
        self.buffer = Buffer(cfg, model=self.model)
        self.total_steps = cfg["total_steps"]
        self.use_wandb = use_wandb

        self.optimizer = torch.optim.Adam(
            self.crosscoder.parameters(),
            lr=cfg["lr"],
            betas=(cfg["beta1"], cfg["beta2"]),
            weight_decay=0.0,  # Explicitly set to 0 as per January 2025 paper
        )</parameter>
        )

        self.scheduler = torch.optim.lr_scheduler.LambdaLR(
            self.optimizer, self.lr_lambda
        )

        self.step_counter: int = 0

        if use_wandb:
            WANDB_DIR.mkdir(exist_ok=True)
            wandb.init(
                project="crosscroders",
                entity="rohan-kathuria-neu",
                config=cfg,
                dir=str(WANDB_DIR),
            )

    def lr_lambda(self, step):
        if step < 0.05 * self.total_steps:
            return step / (0.05 * self.total_steps)
        elif step < 0.8 * self.total_steps:
            return 1.0
        else:
            return 1.0 - (step - 0.8 * self.total_steps) / (0.2 * self.total_steps)

    def get_l1_coeff(self):
        return self.cfg["l_s_coefficient"] * self.step_counter / self.total_steps

    def get_l0_coeff(self):
        if self.step_counter < 0.05 * self.total_steps:
            return (
                self.cfg["l0_coefficient"]
                * self.step_counter
                / (0.04 * self.total_steps)
            )
        else:
            return self.cfg["l0_coefficient"]

    def step(self):
        acts = self.buffer.next()
        acts = acts.to(dtype=self.cfg["dtype"], device=self.cfg["device"])

        lambda_s = self.get_l1_coeff()
        lambda_p = self.cfg["l_p_coefficient"]

        with torch.autocast(device_type="cuda", dtype=torch.bfloat16):
            losses = self.crosscoder.return_loss(acts)

            loss = (
                losses.reconstruction_loss
                + (lambda_s * losses.sparsity_loss)
                + (lambda_p * losses.pre_act_loss)
            )

        loss.backward()
        clip_grad_norm_(self.crosscoder.parameters(), max_norm=1.0)

        self.optimizer.step()
        self.scheduler.step()
        self.optimizer.zero_grad()

        with torch.no_grad():
            preacts, encoded_acts = self.crosscoder.encode(acts)

        mse = losses.reconstruction_loss.item() / (acts.shape[1] * acts.shape[2])

        mean_sparsity_loss = losses.sparsity_loss.item() / self.crosscoder.ae_dim
        mean_pre_act_loss = losses.pre_act_loss.item() / self.crosscoder.ae_dim

        total_avg_loss = (
            mse + (lambda_s * mean_sparsity_loss) + (lambda_p * mean_pre_act_loss)
        )

        loss_dict = {
            "loss": loss.item(),
            "l2_loss (recon)": losses.reconstruction_loss.item(),
            "sparsity_loss": losses.sparsity_loss.item(),
            "pre_act_loss": losses.pre_act_loss.item(),
            "avg_loss/total_scaled_avg": total_avg_loss,
            "avg_loss/mse": mse,
            "avg_loss/mean_sparsity": mean_sparsity_loss,
            "avg_loss/mean_pre_act": mean_pre_act_loss,
            "hyperparams/lambda_s": lambda_s,
            "hyperparams/lambda_p": lambda_p,
            "hyperparams/lr": self.scheduler.get_last_lr()[0],
            "metrics/sparsity": (encoded_acts > 0).float().mean().item(),
        }

        self.step_counter += 1
        return loss_dict

    def log(self, loss_dict):
        if self.use_wandb:
            wandb.log(loss_dict, step=self.step_counter)

    def save(self):
        self.crosscoder.save()

    def train(self):
        self.step_counter = 0
        try:
            for i in tqdm.trange(self.total_steps):
                loss_dict = self.step()
                if i % self.cfg["log_interval"] == 0:
                    self.log(loss_dict)
                if (i + 1) % self.cfg["save_interval"] == 0:
                    print(f"Saving checkpoint at step {i + 1}")
                    self.save()

                if i % (self.cfg["log_interval"] * 10) == 0 and i > 0:
                    print_gpu_memory(tag=f"Before Step {i + 1}")
                    try:
                        analysis = self.analyze()
                        if self.use_wandb:
                            wandb.log(
                                {
                                    "feature_analysis/mean_sparsity": analysis[
                                        "mean_sparsity"
                                    ],
                                    "feature_analysis/sparsity_std": analysis[
                                        "sparsity_std"
                                    ],
                                    "feature_analysis/dead_features": analysis[
                                        "dead_features"
                                    ],
                                    "feature_analysis/max_layer_error": max(
                                        analysis["layer_reconstruction_errors"]
                                    ),
                                },
                                step=self.step_counter,
                            )
                    except Exception as e:
                        print(f"Failed: {e}")

        finally:
            self.save()

    def analyze(self, n_samples=1000):
        sample_acts = self.buffer.next()[:n_samples]
        sample_acts = sample_acts.to(dtype=self.cfg["dtype"], device=self.cfg["device"])

        with torch.no_grad():
            preacts, encoded = self.crosscoder.encode(sample_acts)
            feature_sparsity = (encoded > 0).float().mean(dim=0)

            most_active = torch.argsort(feature_sparsity, descending=True)[:10]
            least_active = torch.argsort(feature_sparsity, descending=False)[:10]

            reconstructed = self.crosscoder.decode(encoded)
            recon_error = F.mse_loss(reconstructed, sample_acts, reduction="none")
            layer_recon_error = recon_error.mean(dim=[0, 2])

            analysis = {
                "mean_sparsity": feature_sparsity.mean().item(),
                "sparsity_std": feature_sparsity.std().item(),
                "most_active_features": most_active.tolist(),
                "least_active_features": least_active.tolist(),
                "layer_reconstruction_errors": layer_recon_error.tolist(),
                "dead_features": (feature_sparsity < 1e-6).sum().item(),
                "total_features": len(feature_sparsity),
            }

            return analysis
